Сначала я установил hadoop, spark и поднял hdfs. Далее, как указано в первом задании создал папку /test и закинул users.parquet в hdfs. 
Установил pySpark и прочитал users.parquet из hdfs. См. скрин 1.
Далее поднял restAPI с помощью FastAPI, создал default json как в указано в третьем задании. 
Добавил парсер nur.kz и фильтр по категориям. 
Добавил кэш, который в течение 10 минут хранит значения в кэше. 
Положил новости по категориям в виде паркета в hdfs и выдавал пользователю данные, считанные с этого паркета.  


В репозитории находятся: 
	- исходный код REST API
	- parquet файл с категориями
	- скриншот pyspark shell
	 
